import os
import torch
import requests
from io import BytesIO
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
from rembg import remove
from dotenv import load_dotenv

load_dotenv()

# fashion-CLIP ëª¨ë¸ ë¡œë“œ
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_NAME = "patrickjohncyh/fashion-clip"

print(f"ğŸ“¦ Fashion-CLIP ëª¨ë¸ ë¡œë“œ ì¤‘... ({DEVICE})")
model = CLIPModel.from_pretrained(MODEL_NAME).to(DEVICE)
processor = CLIPProcessor.from_pretrained(MODEL_NAME)

def get_features_from_url(image_url: str):
    try:
        # 1. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        response = requests.get(image_url, timeout=10)
        img = Image.open(BytesIO(response.content)).convert("RGBA")

        # 2. ë°°ê²½ ì œê±°
        no_bg_img = remove(img)
        
        # 3. ì „ì²˜ë¦¬(224*224 ì‚¬ì´ì¦ˆ & íšŒìƒ‰ ë°°ê²½)
        target_size = (224, 224)
        no_bg_img.thumbnail(target_size, Image.Resampling.LANCZOS)

        analysis_img = Image.new("RGB", target_size, (128, 128, 128))
        analysis_img.paste(
            no_bg_img, 
            ((target_size[0] - no_bg_img.size[0]) // 2, (target_size[1] - no_bg_img.size[1]) // 2), 
            mask=no_bg_img.split()[3]
        )

        # 4. Fashion-CLIP íŠ¹ì§• ì¶”ì¶œ
        inputs = processor(images=analysis_img, return_tensors="pt").to(DEVICE)
        
        with torch.no_grad():
            features = model.get_image_features(**inputs)
        
        # 5. L2 ì •ê·œí™”(ì •í™•ë„ ìœ„í•´) ë° ë¦¬ìŠ¤íŠ¸ ë³€í™˜(Supabaseì— ì €ì¥í•˜ê¸° ìœ„í•´)
        features /= features.norm(dim=-1, keepdim=True)
        
        return features.squeeze().cpu().tolist()

    except Exception as e:
        print(f"âŒ ë²¡í„°í™” ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None